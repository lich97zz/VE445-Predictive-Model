def net(onehots_shape,lr_):  # [73,398]
    onehots_shape = list(onehots_shape)
    input = tf.placeholder(tf.float32, [None] + onehots_shape, name='input')
    input = tf.reshape(input, [-1] + onehots_shape + [1])
    # input = tf.reshape(input, [None, 73, 398, 1])
    conv_num = 32
    label = tf.placeholder(tf.int32, [None], name='label')
    label = tf.one_hot(label, 2)   
    conv1 = tf.keras.layers.Conv2D(conv_num, 5, 1, 'same', activation=tf.nn.relu)(input)
    pool1 = tf.keras.layers.MaxPool2D(2, 2)(conv1)
    conv2 = tf.keras.layers.Conv2D(conv_num, 3, (1, 2), padding='same', activation=tf.nn.relu)(pool1)
    pool2 = tf.keras.layers.MaxPool2D(2, 2)(conv2)
    conv3 = tf.keras.layers.Conv2D(conv_num, 3, 3, padding='same', activation=tf.nn.relu)(pool2)
    conv4 = tf.keras.layers.Conv2D(conv_num, 3, 3, padding='same', activation=tf.nn.relu)(conv3)    
    pool3 = tf.keras.layers.MaxPool2D(2, 2)(conv4)
    
    # print("p3:",pool3.shape)
    flat = tf.reshape(pool3, [-1, 1*3*conv_num])
    output1 = tf.keras.layers.Dense(16, name='output1')(flat)
    output2 = tf.keras.layers.Dense(16, name='output2')(output1)
    output = tf.keras.layers.Dense(2, name='output')(output2)
    loss = tf.losses.softmax_cross_entropy(onehot_labels=label, logits=output) 

    train_op = tf.train.AdamOptimizer(lr_).minimize(loss)
    accuracy = tf.metrics.accuracy(  # return (acc, update_op), and create 2 local variables
        labels=tf.argmax(label, axis=1), predictions=tf.argmax(output, axis=1), )[1]
    recall = tf.metrics.recall(  # return (acc, update_op), and create 2 local variables
        labels=tf.argmax(label, axis=1), predictions=tf.argmax(output, axis=1), )[1]
    precision = tf.metrics.precision(  # return (acc, update_op), and create 2 local variables
        labels=tf.argmax(label, axis=1), predictions=tf.argmax(output, axis=1), )[1]

    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())
    return init_op, train_op, loss, accuracy, recall, precision